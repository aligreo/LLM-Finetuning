# -*- coding: utf-8 -*-
"""finetune-llama-3.2-3B-dpo

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JZRrJ4Z84rceXEzshTHqXPvBBRBSB-vf
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install -U bitsandbytes trl -q

from google.colab import userdata

hf_token = userdata.get('HF_TOKEN')

from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import torch

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

model_name = "meta-llama/Llama-3.2-3b-instruct"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name,
                                             quantization_config=bnb_config,
                                             device_map="auto",
                                             low_cpu_mem_usage=True,
                                             token=hf_token)
#
tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)

model.config.use_cache = False

from peft import LoraConfig, prepare_model_for_kbit_training

model = prepare_model_for_kbit_training(model)

peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    target_modules=['q_proj', 'k_proj','v_proj','o_proj','up_proj','down_proj','gate_proj'],
    bias="none",
    task_type="CAUSAL_LM"
)

# load the preference dataset

from datasets import load_dataset

dataset_name = "Dahoas/synthetic-instruct-gptj-pairwise"
dataset = load_dataset(dataset_name, split="train")

## prepare the dataset

def chatml_format(example):

    # Format instruction
    message = {"role": "user", "content": example['prompt']}
    prompt = tokenizer.apply_chat_template([message], tokenize=False, add_generation_prompt=True)

    # Format chosen answer
    chosen = example['chosen'] + "<|eot_id|>\n"

    # Format rejected answer
    rejected = example['rejected'] + "<|eot_id|>\n"

    return {
        "prompt": prompt,
        "chosen": chosen,
        "rejected": rejected,
    }

original_columns = dataset.column_names

tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"

# Format dataset
dataset = dataset.map(
    chatml_format,
    remove_columns=original_columns
)

dataset[0]

import warnings
warnings.simplefilter("ignore")

from trl import DPOTrainer, DPOConfig

args = DPOConfig(
    output_dir=f"{model_name}-dpo-based-dataset",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    optim="adamw_8bit",
    max_steps=100,
    logging_steps=10,
    learning_rate=2e-5,
    lr_scheduler_type="cosine",
    warmup_ratio=0.03,
    report_to="none",
    run_name=f"{model_name}-finetuning",
    gradient_checkpointing=True,
    beta=0.1,
    max_prompt_length=1024,
    max_length=2048,
)

trainer = DPOTrainer(
    model=model,
    ref_model=None,
    processing_class=tokenizer,
    peft_config=peft_config,
    train_dataset=dataset,
    args=args
)

trainer.train()

trainer.model.save_pretrained(f"{model_name}-dpo")
trainer.processing_class.save_pretrained(f"{model_name}-dpo")

from transformers import pipeline

pipe = pipeline(task='text-generation',
                model="/content/meta-llama/Llama-3.2-3b-instruct-dpo",
                tokenizer="/content/meta-llama/Llama-3.2-3b-instruct-dpo")

out = pipe("amira has a 3 apples. she give her sister 2. then her father give her 5. what the number of apples amira have now?")

print(out[0]['generated_text'])

out = pipe("solve for 4x + 10 = 30")
print(out[0]['generated_text'])