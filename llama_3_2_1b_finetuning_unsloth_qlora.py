# -*- coding: utf-8 -*-
"""llama-3.2-1B-finetuning-unsloth-qlora

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KmvTuh1Mmo1mXWR6elYNxsf_wRQb76bN
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import os
# if "COLAB_" not in "".join(os.environ.keys()):
#     !pip install unsloth
# else:
#     # Do this only in Colab notebooks! Otherwise use pip install unsloth
#     !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo
#     !pip install sentencepiece protobuf "datasets>=3.4.1" huggingface_hub hf_transfer
#     !pip install --no-deps unsloth

from unsloth import FastModel

model_sequence_length = 2048
model, tokenizer = FastModel.from_pretrained(
    model_name="unsloth/Llama-3.2-1B-Instruct",
    max_seq_length=model_sequence_length,
    dtype=None,
    load_in_4bit=True
)

model.config.use_cache = False

# parameter training with Lora

model = FastModel.get_peft_model(
    model,
    r=16,
    lora_alpha=32,
    target_modules=['q_proj','k_proj','v_proj','o_proj','gate_proj','up_proj','down_proj'],
    lora_dropout=0,
    use_rslora=True,
    use_gradient_checkpointing="unsloth",
    bias="none",
    random_state=5413
)

from datasets import load_dataset, Dataset

dataset = load_dataset("vicgalle/alpaca-gpt4", split="train[:10000]")

print(dataset, "\n\n")
print(dataset[0]['text'])

import warnings
warnings.filterwarnings("ignore")

from trl import SFTTrainer, SFTConfig
from unsloth import is_bfloat16_supported

args = SFTConfig(
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    warmup_steps=15,
    max_seq_length=model_sequence_length,
    dataset_text_field="text",
    num_train_epochs=1,
    logging_steps=100,
    learning_rate=5e-5,
    fp16= not is_bfloat16_supported(),
    bf16= is_bfloat16_supported(),
    optim="adamw_8bit",
    weight_decay=0.1,
    lr_scheduler_type="linear",
    seed=6513,
    output_dir="unsloth/Llama-3.2-1B-Instruct-finetune-vicgalle/alpaca-gpt4-dataset",
    report_to="none",
    run_name="unsloth/Llama-3.2-1B-Instruct-finetune-vicgalle/alpaca-gpt4-dataset"
)

trainer= SFTTrainer(
    model=model,
    processing_class=tokenizer,
    train_dataset=dataset,
    args=args
)

trainer.train()

"""## inference"""

inference_model = FastModel.for_inference(model)

def generate_with_llama(prompt):
  messege = [
    {"role":"user", "content":prompt}
]

  input_ids = tokenizer.apply_chat_template(
      messege,
      add_generation_propt=True,
      return_tensors="pt"
  ).to("cuda")

  from transformers import TextStreamer
  streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

  _ = inference_model.generate(input_ids,
                               temperature=0.6,
                               top_p=0.95,
                               top_k=50,
                               streamer=streamer,
                               max_new_tokens=4000,
                               pad_token_id=tokenizer.eos_token_id
                               )

generate_with_llama("solve for 2x + 3 = 12")

generate_with_llama(\
"""think carfully and answer,amira have 3 apples.
 she gave mohamed 2. then ahmed give her 12.
  then she sell 5 to amr. how many apples left with her now""")

eval_dataset = load_dataset("vicgalle/alpaca-gpt4", split="train[15000:15200]")
eval_dataset

def generate_with_llama_vs_right_answer(idx):
  promot = eval_dataset[idx]['text']
  answer = eval_dataset[idx]['output']

  messege = [
    {"role":"user", "content":promot}
]

  input_ids = tokenizer.apply_chat_template(
      messege,
      add_generation_propt=True,
      return_tensors="pt"
  ).to("cuda")

  from transformers import TextStreamer
  streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

  _ = inference_model.generate(input_ids,
                               temperature=0.6,
                               top_p=0.95,
                               top_k=50,
                               streamer=streamer,
                               max_new_tokens=4000,
                               pad_token_id=tokenizer.eos_token_id
                               )
  print("\n\nThe right answer was:\n")
  print(answer)

generate_with_llama_vs_right_answer(0)

generate_with_llama_vs_right_answer(161)

generate_with_llama_vs_right_answer(92)